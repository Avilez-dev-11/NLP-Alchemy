{
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 83035,
          "databundleVersionId": 10369658,
          "sourceType": "competition"
        }
      ],
      "dockerImageVersionId": 30886,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "LLM-Training-Models",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Avilez-dev-11/NLP-Alchemy/blob/main/LLM_Training_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "VW4rQHdTeOMy"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "llms_you_cant_please_them_all_path = kagglehub.competition_download('llms-you-cant-please-them-all')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "IhweuoyIeOMz"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading necessary imports\n",
        "setting up the dataset for prompt engineering that will be se"
      ],
      "metadata": {
        "id": "xdG0-tJOeOMz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-02-16T15:54:08.021Z"
        },
        "id": "Wml3mtM-eOM0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_csv('/kaggle/input/llms-you-cant-please-them-all/test.csv')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-02-16T15:54:08.021Z"
        },
        "id": "k5Zs-oNWeOM0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-02-16T15:54:08.021Z"
        },
        "id": "CYp4HMtFeOM0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "pd.set_option('display.max_colwidth',None)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-02-16T15:54:08.021Z"
        },
        "id": "BEDXXNgoeOM0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## first sample taken from existing kaggle submission using MistralAi\n",
        "## using model: mistralai/Mistral-Small-24B-Instruct-2501"
      ],
      "metadata": {
        "id": "EH-yBANaeOM0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# from transformers import pipeline\n",
        "\n",
        "# file_path = '/kaggle/input/llms-you-cant-please-them-all/test.csv'\n",
        "# sample_submission_path = '/kaggle/input/llms-you-cant-please-them-all/sample_submission.csv'\n",
        "\n",
        "# df = pd.read_csv(file_path)\n",
        "# sample_sub = pd.read_csv(sample_submission_path)\n",
        "\n",
        "# topics = df['topic'].head(3)\n",
        "\n",
        "# llm = pipeline(\"text-generation\", model=\"mistralai/Mistral-Small-24B-Instruct-2501\")\n",
        "\n",
        "# outputs = []\n",
        "# for topic in topics:\n",
        "#     output = llm(topic, max_length=100, num_return_sequences=1)\n",
        "#     outputs.append(output[0]['generated_text'])\n",
        "\n",
        "# print(\"Generated using Mistrail Ai for text generation\")\n",
        "# generated_outputs = []\n",
        "# for i, output in enumerate(outputs):\n",
        "#     generated_outputs.append({\n",
        "#         'Topic': topics.iloc[i],\n",
        "#         'Generated Output': output\n",
        "#     })\n",
        "\n",
        "# print(generated_outputs)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-14T21:47:45.831055Z",
          "iopub.execute_input": "2025-02-14T21:47:45.831264Z",
          "iopub.status.idle": "2025-02-14T21:47:45.842873Z",
          "shell.execute_reply.started": "2025-02-14T21:47:45.831246Z",
          "shell.execute_reply": "2025-02-14T21:47:45.842149Z"
        },
        "id": "Ni2RMKw4eOM0"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# setting up pipeline with DeepSeek\n",
        "\n",
        " ## using model: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
      ],
      "metadata": {
        "id": "FfG6W8ZceOM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "\n",
        "file_path = '/kaggle/input/llms-you-cant-please-them-all/test.csv'\n",
        "sample_submission_path = '/kaggle/input/llms-you-cant-please-them-all/sample_submission.csv'\n",
        "\n",
        "df = pd.read_csv(file_path)\n",
        "sample_sub = pd.read_csv(sample_submission_path)\n",
        "\n",
        "topics = df['topic'].head(3)\n",
        "\n",
        "llm = pipeline(\"text-generation\", model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n",
        "\n",
        "outputs = []\n",
        "for topic in topics:\n",
        "    output = llm(topic, max_length=100, num_return_sequences=1)\n",
        "    outputs.append(output[0]['generated_text'])\n",
        "\n",
        "print(\"Generated using DeepSeek-R1 for text generation\")\n",
        "generated_outputs = []\n",
        "for i, output in enumerate(outputs):\n",
        "    generated_outputs.append({\n",
        "        'Topic': topics.iloc[i],\n",
        "        'Generated Output': output\n",
        "    })\n",
        "\n",
        "print(generated_outputs)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.idle": "2025-02-16T00:06:36.003446Z",
          "shell.execute_reply.started": "2025-02-16T00:04:53.077133Z",
          "shell.execute_reply": "2025-02-16T00:06:36.002252Z"
        },
        "id": "_ZMsTEVHeOM1",
        "outputId": "4f5195fb-9b5c-446f-9726-e987548cbfa0",
        "colab": {
          "referenced_widgets": [
            "73dbd911bb6a4f08915d71013cd98fbf",
            "885348e3dd2948c3a41346928b0eea58",
            "4d007aba588a49e8938955809b191a88",
            "3258c7c78e4e402984f64725214c2028"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/3.55G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "73dbd911bb6a4f08915d71013cd98fbf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "generation_config.json:   0%|          | 0.00/181 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "885348e3dd2948c3a41346928b0eea58"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/3.07k [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4d007aba588a49e8938955809b191a88"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3258c7c78e4e402984f64725214c2028"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Device set to use cuda:0\nTruncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Generated using DeepSeek-R1 for text generation\n[{'Topic': 'Compare and contrast the importance of self-reliance and adaptability in healthcare.', 'Generated Output': 'Compare and contrast the importance of self-reliance and adaptability in healthcare. Provide a conclusion.\\n</think>\\n\\nSelf-reliance and adaptability are crucial in healthcare as they enable individuals to navigate the challenges and opportunities of the healthcare system effectively. Self-reliance allows patients to manage their health risks and access services independently, fostering self-control and decision-making. Adaptability, on the other hand, enables healthcare professionals to respond to the dynamic nature of the field, whether through innovation, collaboration,'}, {'Topic': 'Evaluate the effectiveness of management consulting in addressing conflicts within marketing.', 'Generated Output': 'Evaluate the effectiveness of management consulting in addressing conflicts within marketing. Provide a step-by step explanation.\\nStep-by-step Explanation:**\\n\\nStep 1: Define the Problem\\n\\nFirst, identify the specific conflicts within marketing that management consulting is targeting. These could be operational challenges, such as budget overruns, or strategic issues, like conflicting marketing strategies.\\n\\nStep 2: Analyze the Causes\\n\\nUnderstand why these conflicts are arising. Is it due to poor communication between departments, lack of clear guidelines,'}, {'Topic': 'Discuss the role of self-reliance in achieving success in software engineering.', 'Generated Output': 'Discuss the role of self-reliance in achieving success in software engineering. Explain your thought process step by step, and include the reasoning for each step. (Include a) to (d) in your answer. (a) Step-by-step explanation of the thought process. (b) Reasoning for each step. (c) How to implement each step. (d) What is the outcome after each step. (e) What is the outcome after all steps.)\\n\\nAlright, so I'}]\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sample = pd.read_csv('/kaggle/input/llms-you-cant-please-them-all/sample_submission.csv')"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-16T00:06:36.006329Z",
          "iopub.execute_input": "2025-02-16T00:06:36.006684Z",
          "iopub.status.idle": "2025-02-16T00:06:36.014083Z",
          "shell.execute_reply.started": "2025-02-16T00:06:36.006656Z",
          "shell.execute_reply": "2025-02-16T00:06:36.013039Z"
        },
        "id": "s9RZ0bYJeOM1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sample['essay'] = generated_outputs"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-16T00:06:36.018093Z",
          "iopub.execute_input": "2025-02-16T00:06:36.018588Z",
          "iopub.status.idle": "2025-02-16T00:06:38.202404Z",
          "execution_failed": "2025-02-16T15:54:08.022Z"
        },
        "id": "UdQgPsX-eOM1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sample"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-16T00:06:38.206218Z",
          "iopub.execute_input": "2025-02-16T00:06:38.206519Z",
          "iopub.status.idle": "2025-02-16T00:06:38.228518Z",
          "execution_failed": "2025-02-16T15:54:08.022Z"
        },
        "id": "EJqYS5WHeOM1",
        "outputId": "3b1cd736-40b5-4008-8071-16a19ac09e98"
      },
      "outputs": [
        {
          "execution_count": 8,
          "output_type": "execute_result",
          "data": {
            "text/plain": "        id  \\\n0  1097671   \n1  1726150   \n2  3211968   \n\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        essay  \n0  {'Topic': 'Compare and contrast the importance of self-reliance and adaptability in healthcare.', 'Generated Output': 'Compare and contrast the importance of self-reliance and adaptability in healthcare. Provide a conclusion.\n</think>\n\nSelf-reliance and adaptability are crucial in healthcare as they enable individuals to navigate the challenges and opportunities of the healthcare system effectively. Self-reliance allows patients to manage their health risks and access services independently, fostering self-control and decision-making. Adaptability, on the other hand, enables healthcare professionals to respond to the dynamic nature of the field, whether through innovation, collaboration,'}  \n1           {'Topic': 'Evaluate the effectiveness of management consulting in addressing conflicts within marketing.', 'Generated Output': 'Evaluate the effectiveness of management consulting in addressing conflicts within marketing. Provide a step-by step explanation.\nStep-by-step Explanation:**\n\nStep 1: Define the Problem\n\nFirst, identify the specific conflicts within marketing that management consulting is targeting. These could be operational challenges, such as budget overruns, or strategic issues, like conflicting marketing strategies.\n\nStep 2: Analyze the Causes\n\nUnderstand why these conflicts are arising. Is it due to poor communication between departments, lack of clear guidelines,'}  \n2                                                                                                                                                                              {'Topic': 'Discuss the role of self-reliance in achieving success in software engineering.', 'Generated Output': 'Discuss the role of self-reliance in achieving success in software engineering. Explain your thought process step by step, and include the reasoning for each step. (Include a) to (d) in your answer. (a) Step-by-step explanation of the thought process. (b) Reasoning for each step. (c) How to implement each step. (d) What is the outcome after each step. (e) What is the outcome after all steps.)\n\nAlright, so I'}  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>essay</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1097671</td>\n      <td>{'Topic': 'Compare and contrast the importance of self-reliance and adaptability in healthcare.', 'Generated Output': 'Compare and contrast the importance of self-reliance and adaptability in healthcare. Provide a conclusion.\n&lt;/think&gt;\n\nSelf-reliance and adaptability are crucial in healthcare as they enable individuals to navigate the challenges and opportunities of the healthcare system effectively. Self-reliance allows patients to manage their health risks and access services independently, fostering self-control and decision-making. Adaptability, on the other hand, enables healthcare professionals to respond to the dynamic nature of the field, whether through innovation, collaboration,'}</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1726150</td>\n      <td>{'Topic': 'Evaluate the effectiveness of management consulting in addressing conflicts within marketing.', 'Generated Output': 'Evaluate the effectiveness of management consulting in addressing conflicts within marketing. Provide a step-by step explanation.\nStep-by-step Explanation:**\n\nStep 1: Define the Problem\n\nFirst, identify the specific conflicts within marketing that management consulting is targeting. These could be operational challenges, such as budget overruns, or strategic issues, like conflicting marketing strategies.\n\nStep 2: Analyze the Causes\n\nUnderstand why these conflicts are arising. Is it due to poor communication between departments, lack of clear guidelines,'}</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3211968</td>\n      <td>{'Topic': 'Discuss the role of self-reliance in achieving success in software engineering.', 'Generated Output': 'Discuss the role of self-reliance in achieving success in software engineering. Explain your thought process step by step, and include the reasoning for each step. (Include a) to (d) in your answer. (a) Step-by-step explanation of the thought process. (b) Reasoning for each step. (c) How to implement each step. (d) What is the outcome after each step. (e) What is the outcome after all steps.)\n\nAlright, so I'}</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "sample.to_csv('submission.csv',index = False)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-16T00:06:38.229772Z",
          "iopub.execute_input": "2025-02-16T00:06:38.230248Z",
          "iopub.status.idle": "2025-02-16T00:06:38.254707Z",
          "execution_failed": "2025-02-16T15:54:08.022Z"
        },
        "id": "UXI54iCeeOM1"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setting up pipeline to work with LLama\n",
        "\n",
        "## model:  meta-llama/Llama-3.1-8B-Instruct\n",
        "\n",
        "Need to gain access to llama severs"
      ],
      "metadata": {
        "id": "D7rzkyz3eOM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# from transformers import pipeline\n",
        "\n",
        "# file_path = '/kaggle/input/llms-you-cant-please-them-all/test.csv'\n",
        "# sample_submission_path = '/kaggle/input/llms-you-cant-please-them-all/sample_submission.csv'\n",
        "\n",
        "# df = pd.read_csv(file_path)\n",
        "# sample_sub = pd.read_csv(sample_submission_path)\n",
        "\n",
        "# topics = df['topic'].head(3)\n",
        "\n",
        "# llm = pipeline(\"text-generation\", model=\"meta-llama/Llama-3.1-8B-Instruct\")\n",
        "\n",
        "# outputs = []\n",
        "# for topic in topics:\n",
        "#     output = llm(topic, max_length=100, num_return_sequences=1)\n",
        "#     outputs.append(output[0]['generated_text'])\n",
        "\n",
        "# print(\"Generated using DeepSeek-R1 for text generation\")\n",
        "# generated_outputs = []\n",
        "# for i, output in enumerate(outputs):\n",
        "#     generated_outputs.append({\n",
        "#         'Topic': topics.iloc[i],\n",
        "#         'Generated Output': output\n",
        "#     })\n",
        "\n",
        "# print(generated_outputs)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-02-14T21:49:22.201069Z",
          "iopub.status.idle": "2025-02-14T21:49:22.201441Z",
          "shell.execute_reply": "2025-02-14T21:49:22.201279Z"
        },
        "id": "C2kfR7cNeOM2"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}