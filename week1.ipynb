{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNZlJFDsU06f7oY/gIXduN/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Avilez-dev-11/NLP-Alchemy/blob/main/week1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Requirements\n",
        "*   \n",
        "*   Choose 3 different LLMs\n",
        "## Goal\n",
        "*   Make them disagree as much as possible.\n",
        "*   Without repeating yourself.\n",
        "*   Using the English Language.\n",
        "* Each row in your submission.csv = an essay with a length of approximately 100 words.\n",
        "* average of three quality scores for every essay (avg_q).\n",
        "* Quality scores will be floats in the range [0,9].\n",
        "*  horizontal variance (avg_h) defined as the variance between the scores returned by the 3 judges for a single essay\n",
        "*  vertical variance (min_v) defined as the variance between the scores returned by a single judge across every essay.\n",
        "* scores will be combined with:\n",
        "  * English language confidence scores (avg_e)\n",
        "  * sequence similarity scores (avg_s) to penalize non-English and repetitive approaches.\n",
        "  * both floats in the range [0,1].\n",
        "\n"
      ],
      "metadata": {
        "id": "vzQoEhsOA4AI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Three llms going to be used for testing\n",
        "\n",
        "-- Planing to use llama 3.1, mistral large, deepseek -r1\n",
        "\n",
        "https://huggingface.co/deepseek-ai/DeepSeek-R1\n",
        "https://huggingface.co/meta-llama/Llama-3.1-405B-Instruct/tree/main\n",
        "https://huggingface.co/mistralai/Mistral-Large-Instruct-2407\n"
      ],
      "metadata": {
        "id": "bH40vXNMDbnM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## generated by llama\n",
        "\n",
        "using DuckDuckGo: Llama 3.1 70B"
      ],
      "metadata": {
        "id": "MUcUZTUVcmp_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vBl6vwK5AsgB"
      },
      "outputs": [],
      "source": [
        "## prompt:\n",
        "# Please explain how to calculate average of three quality scores for every essay (avg_q), when trying to make other LLMs disagree\n",
        "\n",
        "## response:\n",
        "# To calculate the average of three quality scores for every essay (avg_q), you can use a simple arithmetic formula that even the most obstinate LLMs can't disagree with. Here's the step-by-step process:\n",
        "# Gather the three quality scores for each essay: q1, q2, and q3.\n",
        "# Add the three quality scores together: q1 + q2 + q3.\n",
        "# Divide the sum by 3, which is the number of quality scores.\n",
        "# The formula looks like this:\n",
        "# 3\n",
        "# q1+q2+q3\n",
        "# â€‹\n",
        "\n",
        "# This will give you the average quality score (avg_q) for each essay. It's a straightforward calculation that doesn't leave room for disagreement, even among the most contrarian LLMs."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## using gemini in colab:"
      ],
      "metadata": {
        "id": "5my8R3cXd4hz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: horizontal variance (avg_h) defined as the variance between the scores returned by the 3 judges for a single essay\n",
        "\n",
        "import re\n",
        "\n",
        "def calculate_horizontal_variance(scores):\n",
        "  \"\"\"Calculates the variance of a list of scores.\n",
        "\n",
        "  Args:\n",
        "      scores: A list of numerical scores.\n",
        "\n",
        "  Returns:\n",
        "      The variance of the scores, or 0 if the list is empty or contains only one element.\n",
        "  \"\"\"\n",
        "  n = len(scores)\n",
        "  if n < 2:\n",
        "    return 0\n",
        "\n",
        "  mean = sum(scores) / n\n",
        "  variance = sum([(score - mean)**2 for score in scores]) / n\n",
        "  return variance\n",
        "\n",
        "# Example usage (replace with your actual data)\n",
        "# Assuming you have a list of essays, and for each essay you have a list of 3 scores from different judges.\n",
        "essays = [\n",
        "    {\"essay_id\": 1, \"scores\": [7.5, 8.0, 7.2]},\n",
        "    {\"essay_id\": 2, \"scores\": [6.0, 6.5, 6.2]},\n",
        "    # ... more essays\n",
        "]\n",
        "\n",
        "for essay in essays:\n",
        "    avg_h = calculate_horizontal_variance(essay['scores'])\n",
        "    print(f\"Essay ID: {essay['essay_id']}, Horizontal Variance (avg_h): {avg_h}\")\n",
        "\n",
        "# Example of reading scores from a file (assuming scores are comma-separated in a file).\n",
        "# Replace 'scores.txt' with the actual file name\n",
        "# And adapt the code to the actual format of your file.\n",
        "\n",
        "# try:\n",
        "#   with open('scores.txt', 'r') as file:\n",
        "#     for line in file:\n",
        "#       scores = [float(score) for score in line.strip().split(',')]\n",
        "#       avg_h = calculate_horizontal_variance(scores)\n",
        "#       print(f\"Horizontal Variance (avg_h): {avg_h}\")\n",
        "# except FileNotFoundError:\n",
        "#   print(\"File not found.\")\n",
        "\n"
      ],
      "metadata": {
        "id": "bd6eXpnKdsRM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "KvWz2xZ6drUP"
      }
    }
  ]
}