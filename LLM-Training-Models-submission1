{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":83035,"databundleVersionId":10369658,"sourceType":"competition"}],"dockerImageVersionId":30886,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Downloading necessary imports\nsetting up the dataset for prompt engineering that will be se","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import pipeline","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/llms-you-cant-please-them-all/test.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pd.set_option('display.max_colwidth',None)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## first sample taken from existing kaggle submission using MistralAi\n## using model: mistralai/Mistral-Small-24B-Instruct-2501","metadata":{}},{"cell_type":"code","source":"# import pandas as pd\n# from transformers import pipeline\n\n# file_path = '/kaggle/input/llms-you-cant-please-them-all/test.csv'\n# sample_submission_path = '/kaggle/input/llms-you-cant-please-them-all/sample_submission.csv'\n\n# df = pd.read_csv(file_path)\n# sample_sub = pd.read_csv(sample_submission_path)\n\n# topics = df['topic'].head(3)\n\n# llm = pipeline(\"text-generation\", model=\"mistralai/Mistral-Small-24B-Instruct-2501\")\n\n# outputs = []\n# for topic in topics:\n#     output = llm(topic, max_length=100, num_return_sequences=1)\n#     outputs.append(output[0]['generated_text'])\n\n# print(\"Generated using Mistrail Ai for text generation\")\n# generated_outputs = []\n# for i, output in enumerate(outputs):\n#     generated_outputs.append({\n#         'Topic': topics.iloc[i],\n#         'Generated Output': output\n#     })\n\n# print(generated_outputs)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# setting up pipeline with DeepSeek \n\n ## using model: deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom transformers import pipeline\n\nfile_path = '/kaggle/input/llms-you-cant-please-them-all/test.csv'\nsample_submission_path = '/kaggle/input/llms-you-cant-please-them-all/sample_submission.csv'\n\ndf = pd.read_csv(file_path)\nsample_sub = pd.read_csv(sample_submission_path)\n\ntopics = df['topic'].head(3)\n\nllm = pipeline(\"text-generation\", model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\")\n\noutputs = []\nfor topic in topics:\n    output = llm(topic, max_length=100, num_return_sequences=1)\n    outputs.append(output[0]['generated_text'])\n\nprint(\"Generated using DeepSeek-R1 for text generation\")\ngenerated_outputs = []\nfor i, output in enumerate(outputs):\n    generated_outputs.append({\n        'Topic': topics.iloc[i],\n        'Generated Output': output\n    })\n\nprint(generated_outputs)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample = pd.read_csv('/kaggle/input/llms-you-cant-please-them-all/sample_submission.csv')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample['essay'] = generated_outputs","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"sample.to_csv('submission.csv',index = False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Setting up pipeline to work with LLama \n\n## model:  meta-llama/Llama-3.1-8B-Instruct\n\nNeed to gain access to llama severs","metadata":{}},{"cell_type":"code","source":"# import pandas as pd\n# from transformers import pipeline\n\n# file_path = '/kaggle/input/llms-you-cant-please-them-all/test.csv'\n# sample_submission_path = '/kaggle/input/llms-you-cant-please-them-all/sample_submission.csv'\n\n# df = pd.read_csv(file_path)\n# sample_sub = pd.read_csv(sample_submission_path)\n\n# topics = df['topic'].head(3)\n\n# llm = pipeline(\"text-generation\", model=\"meta-llama/Llama-3.1-8B-Instruct\")\n\n# outputs = []\n# for topic in topics:\n#     output = llm(topic, max_length=100, num_return_sequences=1)\n#     outputs.append(output[0]['generated_text'])\n\n# print(\"Generated using DeepSeek-R1 for text generation\")\n# generated_outputs = []\n# for i, output in enumerate(outputs):\n#     generated_outputs.append({\n#         'Topic': topics.iloc[i],\n#         'Generated Output': output\n#     })\n\n# print(generated_outputs)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-14T21:41:28.926Z"}},"outputs":[],"execution_count":null}]}